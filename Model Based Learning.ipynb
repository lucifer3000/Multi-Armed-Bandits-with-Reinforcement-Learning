{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c86af5c",
      "metadata": {
        "id": "9c86af5c"
      },
      "outputs": [],
      "source": [
        "# Most of this code is adapted from https://github.com/berkeleydeeprlcourse/homework/\n",
        "\n",
        "#The following function generates a sample from a probability distribution. You may choose to ignore the logic. Just see how to use it.\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "def weighted_choice(v, p):\n",
        "   total = sum(p)\n",
        "   r = random.uniform(0, total)\n",
        "   upto = 0\n",
        "   for c, w in zip(v,p):\n",
        "      if upto + w >= r:\n",
        "         return c\n",
        "      upto += w\n",
        "   assert False, \"Shouldn't get here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7056c5",
      "metadata": {
        "id": "bb7056c5",
        "outputId": "3021f766-cee6-402f-862d-e3af7f843a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#Using the above function to sample from a distribution\n",
        "\n",
        "#6-faced die with equal probabilities\n",
        "Sample_Space=[1,2,3,4,5,6]\n",
        "Prob_Values=[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
        "\n",
        "#Generating a sample\n",
        "weighted_choice(Sample_Space, Prob_Values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aebf473d",
      "metadata": {
        "id": "aebf473d"
      },
      "outputs": [],
      "source": [
        "#Creating a class for MDP environment definition that takes Transition Probability Matrix p(s'|s,a) and reward E[R_{t+1} | s,a,s'] as inputs\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, transition_probs, rewards, initial_state=None):\n",
        "        \"\"\"\n",
        "        Defines an MDP. Compatible with gym Env.\n",
        "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
        "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
        "            For each state and action, probabilities of next states should sum to 1\n",
        "            If a state has no actions available, it is considered terminal\n",
        "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
        "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
        "            The reward for anything not mentioned here is zero.\n",
        "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
        "            By default, picks initial state at random.\n",
        "\n",
        "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
        "\n",
        "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
        "        transition_probs = {\n",
        "              's0':{\n",
        "                'a0': {'s0': 0.5, 's2': 0.5},\n",
        "                'a1': {'s2': 1}\n",
        "              },\n",
        "              's1':{\n",
        "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "                'a1': {'s1': 0.95, 's2': 0.05}\n",
        "              },\n",
        "              's2':{\n",
        "                'a0': {'s0': 0.4, 's1': 0.6},\n",
        "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
        "              }\n",
        "            }\n",
        "        rewards = {\n",
        "            's1': {'a0': {'s0': +5}},\n",
        "            's2': {'a1': {'s0': -1}}\n",
        "        }\n",
        "        \"\"\"\n",
        "        self._check_param_consistency(transition_probs, rewards)\n",
        "        self._transition_probs = transition_probs\n",
        "        self._rewards = rewards  \n",
        "        self._initial_state = initial_state\n",
        "        self.n_states = len(transition_probs)\n",
        "        self.reset()\n",
        "\n",
        "    def get_all_states(self):\n",
        "        \"\"\" return a tuple of all possible states \"\"\"\n",
        "        return tuple(self._transition_probs.keys())\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
        "        return tuple(self._transition_probs.get(state, {}).keys())\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
        "        return len(self.get_possible_actions(state)) == 0\n",
        "\n",
        "    def get_next_states(self, state, action):\n",
        "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
        "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._transition_probs[state][action]\n",
        "\n",
        "    def get_transition_prob(self, state, action, next_state):\n",
        "        \"\"\" return P(next_state | state, action) \"\"\"\n",
        "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
        "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._rewards.get(state, {}).get(action, {}).get(next_state, -0.01) #-0.01 reward for any transaction other than in to terminal states\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" reset the game, return the initial state\"\"\"\n",
        "        if self._initial_state is None:\n",
        "            self._current_state = random.choice(tuple(self._transition_probs.keys()))\n",
        "        elif self._initial_state in self._transition_probs:\n",
        "            self._current_state = self._initial_state\n",
        "        elif callable(self._initial_state):\n",
        "            self._current_state = self._initial_state()\n",
        "        else:\n",
        "            raise ValueError(\"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
        "        return self._current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
        "        possible_states, probs = zip(*self.get_next_states(self._current_state, action).items())\n",
        "        next_state = weighted_choice(possible_states, p=probs)\n",
        "        reward = self.get_reward(self._current_state, action, next_state)\n",
        "        is_done = self.is_terminal(next_state)\n",
        "        self._current_state = next_state\n",
        "        return next_state, reward, is_done, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(\"Currently at %s\" % self._current_state)\n",
        "\n",
        "    def _check_param_consistency(self, transition_probs, rewards):\n",
        "        for state in transition_probs:\n",
        "            assert isinstance(transition_probs[state], dict), \"transition_probs for %s should be a dictionary \" \\\n",
        "                                                              \"but is instead %s\" % (\n",
        "                                                              state, type(transition_probs[state]))\n",
        "            for action in transition_probs[state]:\n",
        "                assert isinstance(transition_probs[state][action], dict), \"transition_probs for %s, %s should be a \" \\\n",
        "                                                                          \"a dictionary but is instead %s\" % (\n",
        "                                                                              state, action,\n",
        "                                                                              type(transition_probs[state, action]))\n",
        "                next_state_probs = transition_probs[state][action]\n",
        "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
        "                sum_probs = sum(next_state_probs.values())\n",
        "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
        "                                                    \"add up to %f (should be 1)\" % (state, action, sum_probs)\n",
        "        for state in rewards:\n",
        "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
        "                                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
        "            for action in rewards[state]:\n",
        "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
        "                                                                 \"a dictionary but is instead %s\" % (\n",
        "                                                                 state, action, type(transition_probs[state, action]))\n",
        "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
        "              \" you will be sent at the first sign of defiance. \"\n",
        "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
        "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3c16bc",
      "metadata": {
        "id": "db3c16bc"
      },
      "outputs": [],
      "source": [
        "#Creating a MDP \n",
        "\n",
        "transition_probs = {0:{'right': {1: 0.8, 0: 0.1, 4: 0.1},'down': {4:0.8, 1: 0.1, 0: 0.1}, 'left':{0:0.9, 4:0.1}, 'up':{0:0.9, 1:0.1}},\n",
        "                    1:{'right': {2: 0.8, 1: 0.2},'down': {1:0.8, 0:0.1, 2: 0.1}, 'left':{0:0.8, 1:0.2}, 'up':{1:0.8, 0:0.1, 2:0.1}},\n",
        "                    2:{'right': {3: 0.8, 2: 0.1, 6: 0.1},'down': {6:0.8, 1: 0.1, 3: 0.1}, 'left':{1:0.8, 2:0.1, 6:0.1}, 'up':{2:0.8, 1:0.1, 3:0.1}},\n",
        "                  3:{},\n",
        "                  4:{'right': {4:0.8, 0: 0.1, 8:0.1},'down': {8:0.8, 4:0.2}, 'left':{4:0.8, 0:0.1, 8:0.1}, 'up':{0:0.8, 4:0.2}},\n",
        "                  6:{'right': {7: 0.8, 2: 0.1, 10: 0.1},'down': {10:0.8, 6: 0.1, 7: 0.1}, 'left':{6:0.8, 2:0.1, 10:0.1}, 'up':{2:0.8, 6:0.1, 7:0.1}},\n",
        "                  7:{},\n",
        "                  8:{'right': {9: 0.8, 4: 0.1, 8: 0.1},'down': {8:0.9, 9: 0.1}, 'left':{8:0.9, 4:0.1}, 'up':{4:0.8, 8:0.1, 9:0.1}},\n",
        "                  9:{'right': {10: 0.8, 9: 0.2},'down': {9:0.8, 10: 0.1, 8: 0.1}, 'left':{8:0.8, 9:0.2}, 'up':{9:0.8, 8:0.1, 10:0.1}},\n",
        "                  10:{'right': {11: 0.8, 10: 0.1, 6: 0.1},'down': {10:0.8, 9: 0.1, 11: 0.1}, 'left':{9:0.8, 10:0.1, 6:0.1}, 'up':{6:0.8, 9:0.1, 11:0.1}},\n",
        "                  11:{'right': {11: 0.9, 7: 0.1},'down': {11:0.9, 10: 0.1}, 'left':{10:0.8, 7:0.1, 11:0.1}, 'up':{7:0.8, 10:0.1, 11:0.1}}\n",
        "                  }\n",
        "\n",
        "#rewards for the Goal and Hole state\n",
        "rewards = {2: {'right': {3:1}, 'up': {3:1}, 'down': {3:1}}, \n",
        "           6: {'right': {7:-1}, 'up': {7:-1}, 'down': {7:-1}},\n",
        "           11: {'right': {7:-1}, 'up': {7:-1}, 'left': {7:-1}}\n",
        "           }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d69e2a9c",
      "metadata": {
        "id": "d69e2a9c"
      },
      "outputs": [],
      "source": [
        "#Intialize an environment\n",
        "env=MDP(transition_probs, rewards,0) #initial state - 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d26571e2",
      "metadata": {
        "id": "d26571e2",
        "outputId": "b738aea0-07f5-4a9d-a845-c792f2b969aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently at 0\n"
          ]
        }
      ],
      "source": [
        "#To know the current state\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Evaluation"
      ],
      "metadata": {
        "id": "2f5CnCnRszHo"
      },
      "id": "2f5CnCnRszHo"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_values(value):  #Function to print the value function\n",
        "    t=0\n",
        "    for i in range(3):\n",
        "        print(\"------------\")\n",
        "        for j in range(4):\n",
        "            if(t==5):\n",
        "                print(\"\", end=\" \")\n",
        "            else:\n",
        "                print(\"%0.2f\" %value[t], end=\" \")\n",
        "            t+=1\n",
        "        print(\"\")\n",
        "    print('\\n')\n",
        "    print('\\n')\n",
        "\n",
        "def print_policy(policy):   #function to print the policy\n",
        "    t=0\n",
        "    for i in range(3):\n",
        "        print(\"------------\")\n",
        "        for j in range(4):\n",
        "            if(t==5 or t==3 or t==7):\n",
        "                print(\"\", end=\" \")\n",
        "            else:\n",
        "                print(list(policy[t].keys())[0], end=\" \")\n",
        "            t+=1\n",
        "        print(\"\")\n",
        "    print('\\n')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "nBqo_Jbd169V"
      },
      "id": "nBqo_Jbd169V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter  = 0  # to test the number of iterations required to converge Policy Iteration"
      ],
      "metadata": {
        "id": "tg099XQ7F7wr"
      },
      "id": "tg099XQ7F7wr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, env, threshold, gamma):\n",
        "  #Value Function is in form of a dictionary with keys as the states and the values as 0\n",
        "    value = {} #initialising value function to 0\n",
        "    for s in env.get_all_states():\n",
        "        value[s] = 0\n",
        "\n",
        "    it = 0\n",
        "    while True:\n",
        "        delta = 0 #to break the loop of policy evaluation if values do not differ much\n",
        "        for s in env.get_all_states():\n",
        "            if not env.is_terminal(s): #if it's a terminal state then value function is 0\n",
        "                old_v = value[s]\n",
        "                new_v = 0\n",
        "                for a in env.get_possible_actions(s): \n",
        "                    for s2 in env.get_all_states():\n",
        "                        action_prob = policy.get(s,{}).get(a,0) # pi(a|s)\n",
        "                        r = env.get_reward(s,a,s2)\n",
        "                        transition_prob = env.get_transition_prob(s,a,s2)  # p(s'|s,a)\n",
        "\n",
        "                        new_v += action_prob*transition_prob*(r + gamma*value[s2])  #Bellman Equation\n",
        "\n",
        "                value[s] = new_v\n",
        "                delta = max(delta, np.abs(old_v - value[s]))  #change in the value function of the state\n",
        "        print(\"Iteration= \", it, \"biggest change= \",  delta)\n",
        "        print_values(value)\n",
        "        it+=1\n",
        "\n",
        "        if(delta<threshold):\n",
        "            global counter \n",
        "            counter += it\n",
        "            break\n",
        "    return value\n",
        "      "
      ],
      "metadata": {
        "id": "Ube4P3_oy2yW"
      },
      "id": "Ube4P3_oy2yW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's say we are given a policy pi in form of a dictionary pi(a|s) - probability of taking an action given a state, to evaluate\n",
        "#Not considering cell 5 as a state since its a wall and we will not get into that state ever \n",
        "policy = {}\n",
        "for s in env.get_all_states():\n",
        "    if not env.is_terminal(s):\n",
        "        policy[s] = dict({np.random.choice(np.asarray(env.get_possible_actions(s))) : 1.0})  #initialising a random policy with probability 1 to take that action\n",
        "\n",
        "print_policy(policy)\n",
        "\n",
        "gamma = 0.9\n",
        "threshold = 1e-3\n",
        "_ = policy_evaluation(policy,env,threshold,gamma)\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clwe47iN2atb",
        "outputId": "c68db655-43fd-417d-e146-737a77083f7a"
      },
      "id": "clwe47iN2atb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "down left down  \n",
            "------------\n",
            "down  right  \n",
            "------------\n",
            "right left down right \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  0 biggest change=  0.7939493200000001\n",
            "------------\n",
            "-0.01 -0.02 0.09 0.00 \n",
            "------------\n",
            "-0.01  -0.79 0.00 \n",
            "------------\n",
            "-0.01 -0.02 -0.01 -0.11 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  1 biggest change=  0.5725473408\n",
            "------------\n",
            "-0.02 -0.03 -0.48 0.00 \n",
            "------------\n",
            "-0.02  -0.85 0.00 \n",
            "------------\n",
            "-0.03 -0.03 -0.03 -0.20 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  2 biggest change=  0.07151489999999999\n",
            "------------\n",
            "-0.03 -0.04 -0.52 0.00 \n",
            "------------\n",
            "-0.03  -0.85 0.00 \n",
            "------------\n",
            "-0.04 -0.04 -0.05 -0.27 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  3 biggest change=  0.057927069\n",
            "------------\n",
            "-0.04 -0.04 -0.53 0.00 \n",
            "------------\n",
            "-0.04  -0.85 0.00 \n",
            "------------\n",
            "-0.05 -0.05 -0.08 -0.33 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  4 biggest change=  0.046920925890000076\n",
            "------------\n",
            "-0.05 -0.05 -0.53 0.00 \n",
            "------------\n",
            "-0.05  -0.86 0.00 \n",
            "------------\n",
            "-0.06 -0.06 -0.10 -0.37 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  5 biggest change=  0.03800594997090001\n",
            "------------\n",
            "-0.06 -0.06 -0.53 0.00 \n",
            "------------\n",
            "-0.06  -0.86 0.00 \n",
            "------------\n",
            "-0.06 -0.07 -0.12 -0.41 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  6 biggest change=  0.03078481947642897\n",
            "------------\n",
            "-0.06 -0.07 -0.53 0.00 \n",
            "------------\n",
            "-0.07  -0.86 0.00 \n",
            "------------\n",
            "-0.07 -0.07 -0.14 -0.44 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  7 biggest change=  0.02493570377590748\n",
            "------------\n",
            "-0.07 -0.07 -0.54 0.00 \n",
            "------------\n",
            "-0.07  -0.86 0.00 \n",
            "------------\n",
            "-0.08 -0.08 -0.16 -0.47 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  8 biggest change=  0.020197920058485086\n",
            "------------\n",
            "-0.08 -0.08 -0.54 0.00 \n",
            "------------\n",
            "-0.08  -0.86 0.00 \n",
            "------------\n",
            "-0.08 -0.08 -0.17 -0.49 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  9 biggest change=  0.016360315247372914\n",
            "------------\n",
            "-0.08 -0.08 -0.54 0.00 \n",
            "------------\n",
            "-0.08  -0.87 0.00 \n",
            "------------\n",
            "-0.08 -0.08 -0.19 -0.50 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  10 biggest change=  0.0132518553503721\n",
            "------------\n",
            "-0.08 -0.08 -0.54 0.00 \n",
            "------------\n",
            "-0.08  -0.87 0.00 \n",
            "------------\n",
            "-0.09 -0.09 -0.20 -0.52 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  11 biggest change=  0.010734002833801326\n",
            "------------\n",
            "-0.09 -0.09 -0.54 0.00 \n",
            "------------\n",
            "-0.09  -0.87 0.00 \n",
            "------------\n",
            "-0.09 -0.09 -0.21 -0.53 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  12 biggest change=  0.008694542295379137\n",
            "------------\n",
            "-0.09 -0.09 -0.54 0.00 \n",
            "------------\n",
            "-0.09  -0.87 0.00 \n",
            "------------\n",
            "-0.09 -0.09 -0.21 -0.54 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  13 biggest change=  0.007042579259257109\n",
            "------------\n",
            "-0.09 -0.09 -0.54 0.00 \n",
            "------------\n",
            "-0.09  -0.87 0.00 \n",
            "------------\n",
            "-0.09 -0.09 -0.22 -0.54 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  14 biggest change=  0.0057044891999982195\n",
            "------------\n",
            "-0.09 -0.09 -0.54 0.00 \n",
            "------------\n",
            "-0.09  -0.87 0.00 \n",
            "------------\n",
            "-0.09 -0.09 -0.23 -0.55 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  15 biggest change=  0.0046206362519986\n",
            "------------\n",
            "-0.09 -0.09 -0.54 0.00 \n",
            "------------\n",
            "-0.09  -0.87 0.00 \n",
            "------------\n",
            "-0.09 -0.09 -0.23 -0.55 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  16 biggest change=  0.003766388068671661\n",
            "------------\n",
            "-0.09 -0.09 -0.54 0.00 \n",
            "------------\n",
            "-0.09  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.23 -0.56 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  17 biggest change=  0.0031155073864614358\n",
            "------------\n",
            "-0.10 -0.10 -0.55 0.00 \n",
            "------------\n",
            "-0.10  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.24 -0.56 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  18 biggest change=  0.002571758139678415\n",
            "------------\n",
            "-0.10 -0.10 -0.55 0.00 \n",
            "------------\n",
            "-0.10  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.24 -0.56 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  19 biggest change=  0.002119151206940506\n",
            "------------\n",
            "-0.10 -0.10 -0.55 0.00 \n",
            "------------\n",
            "-0.10  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.24 -0.57 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  20 biggest change=  0.0017435568793055245\n",
            "------------\n",
            "-0.10 -0.10 -0.55 0.00 \n",
            "------------\n",
            "-0.10  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.24 -0.57 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  21 biggest change=  0.0014326742572413342\n",
            "------------\n",
            "-0.10 -0.10 -0.55 0.00 \n",
            "------------\n",
            "-0.10  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.25 -0.57 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  22 biggest change=  0.001175917323829767\n",
            "------------\n",
            "-0.10 -0.10 -0.55 0.00 \n",
            "------------\n",
            "-0.10  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.25 -0.57 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  23 biggest change=  0.0009642582827260604\n",
            "------------\n",
            "-0.10 -0.10 -0.55 0.00 \n",
            "------------\n",
            "-0.10  -0.87 0.00 \n",
            "------------\n",
            "-0.10 -0.10 -0.25 -0.57 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Improvement"
      ],
      "metadata": {
        "id": "ooLLsaNTEX8w"
      },
      "id": "ooLLsaNTEX8w"
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(policy,env,threshold,gamma):\n",
        "    V = {}\n",
        "    while True:\n",
        "        V = policy_evaluation(policy,env,threshold,gamma)\n",
        "\n",
        "        policy_same = True  #if we get the same policy as before, means policy improvement has converged\n",
        "        for s in env.get_all_states():\n",
        "            if not env.is_terminal(s):\n",
        "                old_a = list(policy[s].keys())[0] #action according to the policy of previous iteration\n",
        "                new_a = None\n",
        "                best_val = float('-inf')  #choosing the best value as the smallest quantity\n",
        "\n",
        "                for a in env.get_possible_actions(s):\n",
        "                    v = 0\n",
        "                    for s2 in env.get_all_states():\n",
        "                        r = env.get_reward(s,a,s2)\n",
        "                        transition_prob = env.get_transition_prob(s,a,s2)  # p(s'|s,a)\n",
        "                        v += transition_prob*(r + gamma*V[s2])\n",
        "                    \n",
        "                    if v > best_val:  #taking the argmax over all the actions and finding the best value\n",
        "                        best_val = v\n",
        "                        new_a = a\n",
        "                \n",
        "                policy[s] = dict({new_a: 1.0})  #updating the policy\n",
        "                if new_a != old_a:\n",
        "                    policy_same = False  #policy has changed for atleast one state hence continue with policy improvement\n",
        "        \n",
        "        if policy_same:\n",
        "            break  \n",
        "    \n",
        "    return V, policy\n"
      ],
      "metadata": {
        "id": "GrdMzCmIL2QT"
      },
      "id": "GrdMzCmIL2QT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = {}\n",
        "for s in env.get_all_states():\n",
        "    if not env.is_terminal(s):\n",
        "        policy[s] = dict({np.random.choice(np.asarray(env.get_possible_actions(s))) : 1.0})\n",
        "\n",
        "print(\"Initial Policy:\")\n",
        "print_policy(policy)\n",
        "# for gamma 1 then takes many iterations - policy evaluation\n",
        "gamma = 0.9\n",
        "threshold = 1e-3\n",
        "V,policy = policy_improvement(policy,env,threshold,gamma)\n",
        "print(\"Final Values:\")\n",
        "print_values(V)\n",
        "print(\"Final Policy:\")\n",
        "print_policy(policy)\n",
        "print(\"Number of iterations required to converge Policy Iteration = \", counter) #total number of iterations in doing policy evaluation for all iterations of policy improvement to get the final policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMhIXe7bEXTh",
        "outputId": "319215e1-84cf-4ce4-8446-ef0b55837bad"
      },
      "id": "CMhIXe7bEXTh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Policy:\n",
            "------------\n",
            "up right right  \n",
            "------------\n",
            "right  down  \n",
            "------------\n",
            "left right up left \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  0 biggest change=  0.798\n",
            "------------\n",
            "-0.01 -0.01 0.80 0.00 \n",
            "------------\n",
            "-0.01  -0.11 0.00 \n",
            "------------\n",
            "-0.01 -0.01 -0.09 -0.17 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  1 biggest change=  0.57276\n",
            "------------\n",
            "-0.02 0.56 0.86 0.00 \n",
            "------------\n",
            "-0.02  -0.18 0.00 \n",
            "------------\n",
            "-0.02 -0.08 -0.16 -0.24 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  2 biggest change=  0.147744\n",
            "------------\n",
            "0.03 0.71 0.86 0.00 \n",
            "------------\n",
            "-0.02  -0.24 0.00 \n",
            "------------\n",
            "-0.03 -0.14 -0.22 -0.29 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  3 biggest change=  0.052070410137600026\n",
            "------------\n",
            "0.07 0.74 0.85 0.00 \n",
            "------------\n",
            "-0.02  -0.29 0.00 \n",
            "------------\n",
            "-0.04 -0.19 -0.26 -0.32 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  4 biggest change=  0.04213104684480001\n",
            "------------\n",
            "0.12 0.74 0.85 0.00 \n",
            "------------\n",
            "-0.02  -0.32 0.00 \n",
            "------------\n",
            "-0.04 -0.23 -0.29 -0.35 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  5 biggest change=  0.03418417242051841\n",
            "------------\n",
            "0.15 0.73 0.85 0.00 \n",
            "------------\n",
            "-0.01  -0.35 0.00 \n",
            "------------\n",
            "-0.04 -0.26 -0.32 -0.37 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  6 biggest change=  0.027400868622376034\n",
            "------------\n",
            "0.18 0.73 0.84 0.00 \n",
            "------------\n",
            "-0.01  -0.37 0.00 \n",
            "------------\n",
            "-0.05 -0.29 -0.33 -0.38 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  7 biggest change=  0.021916629718345654\n",
            "------------\n",
            "0.20 0.73 0.84 0.00 \n",
            "------------\n",
            "-0.00  -0.38 0.00 \n",
            "------------\n",
            "-0.05 -0.30 -0.35 -0.39 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  8 biggest change=  0.017532968805883842\n",
            "------------\n",
            "0.22 0.73 0.84 0.00 \n",
            "------------\n",
            "0.00  -0.39 0.00 \n",
            "------------\n",
            "-0.05 -0.31 -0.36 -0.40 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  9 biggest change=  0.014035391636968986\n",
            "------------\n",
            "0.23 0.72 0.84 0.00 \n",
            "------------\n",
            "0.01  -0.40 0.00 \n",
            "------------\n",
            "-0.05 -0.32 -0.36 -0.41 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  10 biggest change=  0.011243849462263311\n",
            "------------\n",
            "0.24 0.72 0.84 0.00 \n",
            "------------\n",
            "0.01  -0.41 0.00 \n",
            "------------\n",
            "-0.05 -0.33 -0.37 -0.41 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  11 biggest change=  0.009014054948497552\n",
            "------------\n",
            "0.25 0.72 0.84 0.00 \n",
            "------------\n",
            "0.02  -0.41 0.00 \n",
            "------------\n",
            "-0.05 -0.34 -0.37 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  12 biggest change=  0.007231437614777125\n",
            "------------\n",
            "0.26 0.72 0.84 0.00 \n",
            "------------\n",
            "0.02  -0.42 0.00 \n",
            "------------\n",
            "-0.05 -0.34 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  13 biggest change=  0.005805123676008606\n",
            "------------\n",
            "0.26 0.72 0.84 0.00 \n",
            "------------\n",
            "0.03  -0.42 0.00 \n",
            "------------\n",
            "-0.05 -0.34 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  14 biggest change=  0.004662985126524244\n",
            "------------\n",
            "0.27 0.72 0.84 0.00 \n",
            "------------\n",
            "0.03  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.34 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  15 biggest change=  0.0037477121315201978\n",
            "------------\n",
            "0.27 0.72 0.84 0.00 \n",
            "------------\n",
            "0.03  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  16 biggest change=  0.003013718357076589\n",
            "------------\n",
            "0.28 0.72 0.84 0.00 \n",
            "------------\n",
            "0.03  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  17 biggest change=  0.002424703607871559\n",
            "------------\n",
            "0.28 0.72 0.84 0.00 \n",
            "------------\n",
            "0.04  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  18 biggest change=  0.0019517322294757555\n",
            "------------\n",
            "0.28 0.72 0.84 0.00 \n",
            "------------\n",
            "0.04  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  19 biggest change=  0.0015717161642482824\n",
            "------------\n",
            "0.28 0.72 0.84 0.00 \n",
            "------------\n",
            "0.04  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.38 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  20 biggest change=  0.0012662158458466855\n",
            "------------\n",
            "0.28 0.72 0.84 0.00 \n",
            "------------\n",
            "0.04  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.39 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  21 biggest change=  0.0010617749427797168\n",
            "------------\n",
            "0.28 0.72 0.83 0.00 \n",
            "------------\n",
            "0.04  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.39 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  22 biggest change=  0.0009127907261048118\n",
            "------------\n",
            "0.29 0.72 0.83 0.00 \n",
            "------------\n",
            "0.04  -0.42 0.00 \n",
            "------------\n",
            "-0.04 -0.35 -0.39 -0.42 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  0 biggest change=  0.798\n",
            "------------\n",
            "-0.01 -0.01 0.80 0.00 \n",
            "------------\n",
            "-0.02  0.47 0.00 \n",
            "------------\n",
            "-0.02 -0.03 0.01 -0.01 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  1 biggest change=  0.57276\n",
            "------------\n",
            "-0.02 0.56 0.91 0.00 \n",
            "------------\n",
            "-0.03  0.59 0.00 \n",
            "------------\n",
            "-0.03 -0.04 0.02 -0.02 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  2 biggest change=  0.41061504960000006\n",
            "------------\n",
            "0.39 0.75 0.93 0.00 \n",
            "------------\n",
            "0.27  0.62 0.00 \n",
            "------------\n",
            "0.18 0.11 0.13 -0.01 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  3 biggest change=  0.19658287156608012\n",
            "------------\n",
            "0.59 0.80 0.94 0.00 \n",
            "------------\n",
            "0.46  0.62 0.00 \n",
            "------------\n",
            "0.35 0.26 0.24 0.00 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  4 biggest change=  0.0924056145067409\n",
            "------------\n",
            "0.66 0.81 0.94 0.00 \n",
            "------------\n",
            "0.55  0.62 0.00 \n",
            "------------\n",
            "0.44 0.35 0.32 0.02 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  5 biggest change=  0.0449050100859561\n",
            "------------\n",
            "0.68 0.81 0.94 0.00 \n",
            "------------\n",
            "0.58  0.62 0.00 \n",
            "------------\n",
            "0.48 0.40 0.36 0.04 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  6 biggest change=  0.019056977409599496\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.49 0.42 0.38 0.06 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  7 biggest change=  0.013994154553002444\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.42 0.39 0.07 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  8 biggest change=  0.011566319850465356\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.08 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  9 biggest change=  0.00945071685000995\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.09 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  10 biggest change=  0.0076831123308062355\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.10 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  11 biggest change=  0.0062326294196773935\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.11 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  12 biggest change=  0.005051450411921565\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.11 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  13 biggest change=  0.004092636934989846\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.11 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  14 biggest change=  0.0033153377233562653\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.12 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  15 biggest change=  0.002685517040287133\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.12 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  16 biggest change=  0.0021752974535075853\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.12 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  17 biggest change=  0.0017619996395021043\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.12 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  18 biggest change=  0.0014272223308376686\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.13 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  19 biggest change=  0.0011560508732723906\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.13 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  20 biggest change=  0.0009364014411188393\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.39 0.13 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  0 biggest change=  0.798\n",
            "------------\n",
            "-0.01 -0.01 0.80 0.00 \n",
            "------------\n",
            "-0.02  0.47 0.00 \n",
            "------------\n",
            "-0.02 -0.03 0.32 0.12 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  1 biggest change=  0.57276\n",
            "------------\n",
            "-0.02 0.56 0.91 0.00 \n",
            "------------\n",
            "-0.03  0.59 0.00 \n",
            "------------\n",
            "-0.03 -0.04 0.42 0.21 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  2 biggest change=  0.41061504960000006\n",
            "------------\n",
            "0.39 0.75 0.93 0.00 \n",
            "------------\n",
            "0.27  0.62 0.00 \n",
            "------------\n",
            "0.18 0.11 0.46 0.24 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  3 biggest change=  0.19658287156608012\n",
            "------------\n",
            "0.59 0.80 0.94 0.00 \n",
            "------------\n",
            "0.46  0.62 0.00 \n",
            "------------\n",
            "0.35 0.26 0.48 0.26 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  4 biggest change=  0.0924056145067409\n",
            "------------\n",
            "0.66 0.81 0.94 0.00 \n",
            "------------\n",
            "0.55  0.62 0.00 \n",
            "------------\n",
            "0.44 0.35 0.49 0.27 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  5 biggest change=  0.0449050100859561\n",
            "------------\n",
            "0.68 0.81 0.94 0.00 \n",
            "------------\n",
            "0.58  0.62 0.00 \n",
            "------------\n",
            "0.48 0.40 0.50 0.27 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  6 biggest change=  0.019056977409599496\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.49 0.42 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  7 biggest change=  0.007404104841235104\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.42 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  8 biggest change=  0.002704714385745821\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  9 biggest change=  0.0009444368658355629\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  0 biggest change=  0.798\n",
            "------------\n",
            "-0.01 -0.01 0.80 0.00 \n",
            "------------\n",
            "-0.02  0.47 0.00 \n",
            "------------\n",
            "-0.02 -0.01 0.32 0.12 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  1 biggest change=  0.57276\n",
            "------------\n",
            "-0.02 0.56 0.91 0.00 \n",
            "------------\n",
            "-0.03  0.59 0.00 \n",
            "------------\n",
            "-0.03 0.22 0.45 0.22 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  2 biggest change=  0.41061504960000006\n",
            "------------\n",
            "0.39 0.75 0.93 0.00 \n",
            "------------\n",
            "0.27  0.62 0.00 \n",
            "------------\n",
            "0.20 0.35 0.49 0.26 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  3 biggest change=  0.19658287156608012\n",
            "------------\n",
            "0.59 0.80 0.94 0.00 \n",
            "------------\n",
            "0.46  0.62 0.00 \n",
            "------------\n",
            "0.37 0.40 0.50 0.27 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  4 biggest change=  0.08557362537254348\n",
            "------------\n",
            "0.66 0.81 0.94 0.00 \n",
            "------------\n",
            "0.55  0.62 0.00 \n",
            "------------\n",
            "0.45 0.42 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  5 biggest change=  0.031763550702447574\n",
            "------------\n",
            "0.68 0.81 0.94 0.00 \n",
            "------------\n",
            "0.58  0.62 0.00 \n",
            "------------\n",
            "0.48 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  6 biggest change=  0.011042497087152936\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  7 biggest change=  0.0035846738642204\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  8 biggest change=  0.001112191707441812\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Iteration=  9 biggest change=  0.0003344720391862399\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Final Values:\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Final Policy:\n",
            "------------\n",
            "right right right  \n",
            "------------\n",
            "up  up  \n",
            "------------\n",
            "up right up left \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Number of iterations required to converge Policy Iteration =  274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration"
      ],
      "metadata": {
        "id": "nGs6lE_wMSyq"
      },
      "id": "nGs6lE_wMSyq"
    },
    {
      "cell_type": "code",
      "source": [
        "#Value iteration focuses on getting the best value function first and then computing the best policy from it\n",
        "V = {}\n",
        "for s in env.get_all_states():\n",
        "    V[s] = 0  #initialising values of the value function to 0\n",
        "\n",
        "gamma = 0.9\n",
        "threshold = 1e-3\n",
        "it = 0\n",
        "while True:\n",
        "    delta = 0\n",
        "    for s in env.get_all_states():\n",
        "        if not env.is_terminal(s):\n",
        "            old_v = V[s]\n",
        "            new_v = float('-inf')\n",
        "\n",
        "            for a in env.get_possible_actions(s):\n",
        "                v = 0\n",
        "                for s2 in env.get_all_states():\n",
        "                    r = env.get_reward(s,a,s2)\n",
        "                    transition_prob = env.get_transition_prob(s,a,s2)  # p(s'|s,a)\n",
        "                    v += transition_prob*(r + gamma*V[s2])\n",
        "\n",
        "                if(v > new_v): #computing the max and getting the best value over all the actions\n",
        "                    new_v = v\n",
        "            \n",
        "            V[s] = new_v    \n",
        "            delta = max(delta, np.abs(old_v - V[s]))\n",
        "\n",
        "    it+=1\n",
        "    if delta < threshold :  #breaking the loop if not much change in the values\n",
        "        break\n",
        "\n",
        "#Finding the policy according to the optimal value function\n",
        "policy = {}\n",
        "for s in env.get_all_states():\n",
        "    if not env.is_terminal(s):\n",
        "        best_a = None\n",
        "        best_val = float('-inf')\n",
        "\n",
        "        for a in env.get_possible_actions(s):\n",
        "            v = 0\n",
        "            for s2 in env.get_all_states():\n",
        "                r = env.get_reward(s,a,s2)\n",
        "                transition_prob = env.get_transition_prob(s,a,s2)\n",
        "                v += transition_prob*(r + gamma*V[s2])\n",
        "            \n",
        "            if (v>best_val): #choosing the best action over all the actions from the optimal value function from the state s\n",
        "                best_val = v\n",
        "                best_a = a\n",
        "        \n",
        "        policy[s] = dict({best_a: 1.0})\n",
        "\n",
        "print(\"Optimal Value Function:\")\n",
        "print_values(V)\n",
        "print(\"Final Policy:\")\n",
        "print_policy(policy)\n",
        "print(\"Number of iterations required to converge Value Iteration = \",it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3BX_O-i2diM",
        "outputId": "23683282-11fe-44a5-a506-3daec4f9d139"
      },
      "id": "m3BX_O-i2diM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "------------\n",
            "0.69 0.81 0.94 0.00 \n",
            "------------\n",
            "0.59  0.62 0.00 \n",
            "------------\n",
            "0.50 0.43 0.50 0.28 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Final Policy:\n",
            "------------\n",
            "right right right  \n",
            "------------\n",
            "up  up  \n",
            "------------\n",
            "up right up left \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Number of iterations required to converge Value Iteration =  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparison in terms of Convergence\n",
        "\n",
        "I have used the 'counter' variable in Policy Iteration and 'it' variable in Value Iteration. On seeing the values of the two variables, it can be seen that Policy Iteration requires much more number of iterations to converge compared to Value Iteration. Policy Iteration loop uses Policy Evaluation which can go on for a long time, so Value Iteration forgets about the policy and just computes the max and gets the Value Function. Value Iteration does not evaluates every new policy fully but does only one iteration. Hence, Value Iteration is faster than Policy Iteration. \n",
        "\n",
        "Yes, the optimal policies obtained by both the algorithms is the same."
      ],
      "metadata": {
        "id": "aC9EW_vlKKDI"
      },
      "id": "aC9EW_vlKKDI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Name : Rishabh Katiyar\n",
        "###Roll No. : 190702"
      ],
      "metadata": {
        "id": "RXQhZnRwh75Z"
      },
      "id": "RXQhZnRwh75Z"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CUJUwGTSrkZ"
      },
      "id": "2CUJUwGTSrkZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gym",
      "language": "python",
      "name": "gym"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}